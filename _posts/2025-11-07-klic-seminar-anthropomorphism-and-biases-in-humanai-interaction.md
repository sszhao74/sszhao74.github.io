---
layout: post
title: Anthropomorphism and Biases in Human–AI Interaction
date: 2025-11-07
description: On November 6, 2025, the Kowloon Interaction Design Center (KLIC) at City University of Hong Kong hosted a seminar titled “Anthropomorphism and Biases in Human–AI Interaction.”
tags: research session
categories: session
thumbnail: assets/img/posts/Image_20251107170838.jpg
images:
  lightbox2: true
  photoswipe: true
  spotlight: true
  venobox: true
---

<div class="post_img">
  <img src="/assets/img/posts/Image_20251107170838.jpg" alt="" width="1000"/>
</div>

On **November 6, 2025**, the **Kowloon Interaction Design Center (KLIC)** at **City University of Hong Kong** hosted a seminar titled *“Anthropomorphism and Biases in Human–AI Interaction.”* The talk was delivered by **Dr. Manuele Reani**, **Assistant Professor** at the **School of Management and Economics, The Chinese University of Hong Kong (Shenzhen)**.  

The seminar examined how anthropomorphic design, the attribution of human-like characteristics to artificial systems, affects user perception, trust, and decision-making in interactions with AI.  
<p></p>
<div class="post_img">
  <img src="/assets/img/posts/Image_20251107170831.jpg" alt="" width="1000"/>
</div>
<p></p>

Dr. Reani presented findings from two complementary studies. The first introduced the **Fundamental Over-Attribution Error (FOE)**, a cognitive bias describing how users project human-like traits and intentions onto AI systems, particularly under risk and uncertainty. The study used multiple research methods, including experiments, surveys, and psychometric scale development, to establish FOE as a distinct construct that influences perceptions of AI reliability and intent.  

The second study explored how anthropomorphic design impacts trust and risk perception in **AI-assisted decision-making**, based on a large-scale experiment involving financial advisory chatbots. Results showed that anthropomorphic features reduced perceived risk by increasing both cognitive and affective trust. These effects were moderated by users’ domain expertise. Participants with lower expertise were more prone to misplaced trust, while those with higher expertise responded more critically.  

Dr. Reani concluded by discussing the implications of these findings for **responsible AI design**, emphasizing the importance of calibrating user trust and mitigating bias when developing anthropomorphic decision-support systems.  

**Time:** November 6, 2025 (Thursday), 3:00–4:00 PM (BJT)  
**Venue:** SM6058, City University of Hong Kong  


### About the Speaker  
<p></p>
<div class="post_img">
  <img src="/assets/img/posts/manuele-reani.jpg" alt="" width="1000"/>
</div>
<p></p>
Dr. Manuele Reani is an Assistant Professor at the School of Management and Economics, The Chinese University of Hong Kong (Shenzhen). His research focuses on Human–Computer Interaction, Cognitive Science, and AI Design, with particular interest in how intelligent systems influence human cognition, trust, and decision-making.  

He holds a PhD in Computer Science from the University of Manchester, an MSc in Management Science from the London School of Economics, and a BSc in Psychology from City University London. His work has been published in leading journals and conferences including *IJHCS*, *Computers in Human Behavior*, *CHI*, and *CSCW*.  
